{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Train\n",
    "\"\"\"\n",
    "from datetime import datetime\n",
    "from time import time\n",
    "import numpy as np\n",
    "import shutil, random, os, sys, torch\n",
    "from glob import glob\n",
    "import wandb\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "prj_dir = os.getcwd()\n",
    "sys.path.append(prj_dir)\n",
    "\n",
    "from modules.utils import load_yaml, get_logger\n",
    "from modules.metrics import get_metric_function\n",
    "from modules.earlystoppers import EarlyStopper\n",
    "from modules.losses import get_loss_function\n",
    "from modules.optimizers import get_optimizer\n",
    "from modules.schedulers import get_scheduler\n",
    "from modules.scalers import get_image_scaler\n",
    "from modules.transforms import get_transform_function\n",
    "from modules.datasets import get_dataset_function\n",
    "from modules.recorders import Recorder\n",
    "from modules.trainer import Trainer\n",
    "from models.utils import get_model\n",
    "\n",
    "# Load config\n",
    "config_path = os.path.join(prj_dir, 'config', 'train.yaml')\n",
    "config = load_yaml(config_path)\n",
    "\n",
    "# Set train serial: ex) 20211004\n",
    "train_serial = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "train_serial = 'debug' if config['debug'] else train_serial\n",
    "\n",
    "wandb.init(\n",
    "        project=config['project_name'],\n",
    "        config={\n",
    "            \"architecture\": config['model']['model_name'],\n",
    "            \"dataset\": config['dataset_name'],\n",
    "            \"notes\": config['wandb_note'],\n",
    "        },\n",
    "        name=config['run_name'],\n",
    ")\n",
    "\n",
    "# Set random seed, deterministic\n",
    "torch.cuda.manual_seed(config['seed'])\n",
    "torch.manual_seed(config['seed'])\n",
    "np.random.seed(config['seed'])\n",
    "random.seed(config['seed'])\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Set device(GPU/CPU)\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(config['gpu_num'])\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Create train result directory and set logger\n",
    "train_result_dir = os.path.join(prj_dir, 'results', 'train', train_serial)\n",
    "os.makedirs(train_result_dir, exist_ok=True)\n",
    "\n",
    "# Set logger\n",
    "logging_level = 'debug' if config['verbose'] else 'info'\n",
    "logger = get_logger(name='train',\n",
    "                    file_path=os.path.join(train_result_dir, 'train.log'),\n",
    "                    level=logging_level)\n",
    "\n",
    "\n",
    "# Set data directory\n",
    "train_dirs = os.path.join(prj_dir, 'data', 'train')\n",
    "\n",
    "# Load data and create dataset for train \n",
    "# Load image scaler\n",
    "# train_img_paths = glob(os.path.join(train_dirs, 'x', '*.png'))\n",
    "# train_img_paths, val_img_paths = train_test_split(train_img_paths, test_size=config['val_size'], random_state=config['seed'], shuffle=True)\n",
    "transforms = get_transform_function(config['transform_name'],config=config)\n",
    "\n",
    "train_dataset = get_dataset_function(config['dataset_name'])\n",
    "val_dataset = get_dataset_function(config['dataset_name'])\n",
    "train_dataset = train_dataset(config['train_data_path'],transform=transforms)\n",
    "val_dataset = val_dataset(config['val_data_path'],transform=transforms)\n",
    "\n",
    "# train_dataset = train_dataset(paths=train_img_paths,\n",
    "#                         input_size=[config['input_width'], config['input_height']],\n",
    "#                         scaler=get_image_scaler(config['scaler']),\n",
    "#                         logger=logger)\n",
    "# val_dataset = val_dataset(paths=val_img_paths,\n",
    "#                         input_size=[config['input_width'], config['input_height']],\n",
    "#                         scaler=get_image_scaler(config['scaler']),\n",
    "                        # logger=logger)\n",
    "# Create data loader\n",
    "train_dataloader = DataLoader(dataset=train_dataset,\n",
    "                            batch_size=config['batch_size'],\n",
    "                            num_workers=config['num_workers'], \n",
    "                            shuffle=config['shuffle'],\n",
    "                            drop_last=config['drop_last'])\n",
    "                            \n",
    "val_dataloader = DataLoader(dataset=val_dataset,\n",
    "                            batch_size=config['batch_size'],\n",
    "                            num_workers=config['num_workers'], \n",
    "                            shuffle=False,\n",
    "                            drop_last=config['drop_last'])\n",
    "\n",
    "logger.info(f\"Load dataset, train: {len(train_dataset)}, val: {len(val_dataset)}\")\n",
    "\n",
    "# Load model\n",
    "model = get_model(model_str=config['model']['model_name'])\n",
    "model = model(**config['model']['args']).to(device)\n",
    "logger.info(f\"Load model architecture: {config['model']['model_name']}\")\n",
    "\n",
    "# Set optimizer\n",
    "optimizer = get_optimizer(optimizer_str=config['optimizer']['name'])\n",
    "optimizer = optimizer(model.parameters(), **config['optimizer']['args'])\n",
    "\n",
    "# Set Scheduler\n",
    "scheduler = get_scheduler(scheduler_str=config['scheduler']['name'])\n",
    "scheduler = scheduler(optimizer=optimizer, **config['scheduler']['args'])\n",
    "\n",
    "# Set loss function\n",
    "loss_func = get_loss_function(loss_function_str=config['loss']['name'])\n",
    "loss_func = loss_func(**config['loss']['args'])\n",
    "\n",
    "# Set metric\n",
    "metric_funcs = {metric_name:get_metric_function(metric_name,device) for metric_name in config['metrics']}\n",
    "logger.info(f\"Load optimizer:{config['optimizer']['name']}, scheduler: {config['scheduler']['name']}, loss: {config['loss']['name']}, metric: {config['metrics']}\")\n",
    "\n",
    "# Set trainer\n",
    "trainer = Trainer(model=model,\n",
    "                optimizer=optimizer,\n",
    "                scheduler=scheduler,\n",
    "                loss_func=loss_func,\n",
    "                metric_funcs=metric_funcs,\n",
    "                device=device,\n",
    "                logger=logger)\n",
    "logger.info(f\"Load trainer\")\n",
    "\n",
    "# Set early stopper\n",
    "early_stopper = EarlyStopper(patience=config['earlystopping_patience'],\n",
    "                            logger=logger)\n",
    "# Set recorder\n",
    "recorder = Recorder(record_dir=train_result_dir,\n",
    "                    model=model,\n",
    "                    optimizer=optimizer,\n",
    "                    scheduler=scheduler,\n",
    "                    logger=logger)\n",
    "logger.info(\"Load early stopper, recorder\")\n",
    "\n",
    "# Recorder - save train config\n",
    "shutil.copy(config_path, os.path.join(recorder.record_dir, 'train.yaml'))\n",
    "\n",
    "# Train\n",
    "print(\"START TRAINING\")\n",
    "logger.info(\"START TRAINING\")\n",
    "for epoch_id in range(config['n_epochs']):\n",
    "    \n",
    "    # Initiate result row\n",
    "    row = dict()\n",
    "    row['epoch_id'] = epoch_id\n",
    "    row['train_serial'] = train_serial\n",
    "    row['lr'] = trainer.scheduler.get_last_lr()\n",
    "\n",
    "    # Train\n",
    "    print(f\"Epoch {epoch_id}/{config['n_epochs']} Train..\")\n",
    "    logger.info(f\"Epoch {epoch_id}/{config['n_epochs']} Train..\")\n",
    "    tic = time()\n",
    "    trainer.train(dataloader=train_dataloader, epoch_index=epoch_id)\n",
    "    toc = time()\n",
    "    # Write tarin result to result row\n",
    "    row['train_loss'] = trainer.loss  # Loss\n",
    "    for metric_name, metric_score in trainer.scores.items():\n",
    "        row[f'train_{metric_name}'] = metric_score\n",
    "\n",
    "    row['train_elapsed_time'] = round(toc-tic, 1)\n",
    "    # Clear\n",
    "    trainer.clear_history()\n",
    "\n",
    "    # Validation\n",
    "    print(f\"Epoch {epoch_id}/{config['n_epochs']} Validation..\")\n",
    "    logger.info(f\"Epoch {epoch_id}/{config['n_epochs']} Validation..\")\n",
    "    tic = time()\n",
    "    trainer.validate(dataloader=val_dataloader, epoch_index=epoch_id)\n",
    "    toc = time()\n",
    "    row['val_loss'] = trainer.loss\n",
    "    # row[f\"val_{config['metric']}\"] = trainer.score\n",
    "    for metric_name, metric_score in trainer.scores.items():\n",
    "        row[f'val_{metric_name}'] = metric_score\n",
    "    row['val_elapsed_time'] = round(toc-tic, 1)\n",
    "    trainer.clear_history()\n",
    "\n",
    "    # Performance record - row\n",
    "    recorder.add_row(row)\n",
    "    recorder.wandb_mlflow_log(model=model, log_dict=row, sample_image=None)\n",
    "    \n",
    "    # Performance record - plot\n",
    "    recorder.save_plot(config['plot'])\n",
    "\n",
    "    # Check early stopping\n",
    "    early_stopper.check_early_stopping(row[config['earlystopping_target']])\n",
    "    if early_stopper.patience_counter == 0:\n",
    "        recorder.save_weight(epoch=epoch_id)\n",
    "        \n",
    "    if early_stopper.stop:\n",
    "        print(f\"Epoch {epoch_id}/{config['n_epochs']}, Stopped counter {early_stopper.patience_counter}/{config['earlystopping_patience']}\")\n",
    "        logger.info(f\"Epoch {epoch_id}/{config['n_epochs']}, Stopped counter {early_stopper.patience_counter}/{config['earlystopping_patience']}\")\n",
    "        break\n",
    "\n",
    "print(\"END TRAINING\")\n",
    "logger.info(\"END TRAINING\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q \"openvino>=2023.1.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = '/home/hojun/Documents/project/ma2024/pytorch_template/results/train/20240411_075053/'\n",
    "val_path = \"/home/hojun/Documents/project/ma2024/pytorch_template/dataset/val/\"\n",
    "test_path = \"/home/hojun/Documents/project/ma2024/pytorch_template/dataset/test/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 967 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "test_set = test_datagen.flow_from_directory(val_path,\n",
    "                                            target_size=(224, 224),\n",
    "                                            batch_size=1,\n",
    "                                            shuffle=True,\n",
    "                                            class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# key 와 value 값을 바꾸어 줍니다.\n",
    "class4 = dict()\n",
    "for key,value in test_set.class_indices.items():\n",
    "    class4[value] = key\n",
    "\n",
    "with open(BASE_PATH+'bin/class4.pickle', 'wb') as f:\n",
    "    pickle.dump(class4, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openvino\n",
    "import openvino as ov\n",
    "from openvino.tools.mo import convert_model\n",
    "import torch\n",
    "BASE_PATH = '/home/hojun/Documents/project/ma2024/pytorch_template/results/train/20240411_075053/'\n",
    "model = torch.load(BASE_PATH+'model.pth')\n",
    "model.eval()\n",
    "ov_model = convert_model(model)\n",
    "ov.save_model(ov_model, BASE_PATH+'bin/model.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------- Step 1. Initialize OpenVINO Runtime Core ------------------------------------------------\n",
    "core = ov.Core()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------- Step 2. Read a model --------------------------------------------------------------------\n",
    "# (.xml and .bin files) or (.onnx file)\n",
    "model = core.read_model(BASE_PATH+'bin/model.xml')\n",
    "\n",
    "if len(model.inputs) != 1: print('Sample supports only single input topologies')\n",
    "if len(model.outputs) != 1: print('Sample supports only single output topologies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "directory_path = \"/home/hojun/Documents/project/ma2024/pytorch_template/dataset/test\"\n",
    "#directory_path = \"./drive/MyDrive/Colab Notebooks/data/test\"\n",
    "\n",
    "# Get a list of all files in the directory\n",
    "file_list = [f for f in os.listdir(test_path) if os.path.isfile(os.path.join(test_path, f))]\n",
    "\n",
    "# Print the list of files\n",
    "print(\"List of files in the directory:\")\n",
    "for file in file_list:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(BASE_PATH+'bin/class4.pickle', 'rb') as f:\n",
    "    labels = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# --------------------------- Step 4. Loading model to the device -----------------------------------------------------\n",
    "print('Loading the model to the plugin')\n",
    "compiled_model = core.compile_model(model, 'CPU')\n",
    "img_height = 224\n",
    "\n",
    "result = []\n",
    "\n",
    "res = open(BASE_PATH+\"result.txt\", \"w\")\n",
    "\n",
    "print('Starting inference in synchronous mode')\n",
    "cnt = 0;\n",
    "\n",
    "for file in file_list:\n",
    "    # --------------------------- Step 5. Set up input --------------------------------------------------------------------\n",
    "    # Read input image\n",
    "    image_path = test_path + \"/\" + file\n",
    "    org_image = cv2.imread(image_path)\n",
    "    image = cv2.resize(org_image, (img_height,img_height))\n",
    "    #img = cv2.resize(img, (224,224), fx=0.5, fy=0.5, interpolation=cv2.INTER_AREA)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    # image = image / 255.\n",
    "\n",
    "    # Add N dimension\n",
    "    nchw_tensor = np.expand_dims(image, 0)\n",
    "\n",
    "    # NHWC -> NCHW\n",
    "    input_tensor = np.transpose(nchw_tensor, (0,3,1,2))#(0,3,1,2))\n",
    "    \n",
    "\n",
    "    # --------------------------- Step 6. Create infer request and do inference synchronously -----------------------------\n",
    "    results = compiled_model.infer_new_request({0:input_tensor})\n",
    "\n",
    "    # --------------------------- Step 7. Process output ------------------------------------------------------------------\n",
    "    predictions = next(iter(results.values()))\n",
    "    output_node_name = next(iter(results.keys())) #\n",
    "    #for (k,v) in results.items():\n",
    "    #  print(k,'=',v)\n",
    "\n",
    "    #print(\"values: \",end=' ')\n",
    "    #print(results.values())\n",
    "\n",
    "    # Change a shape of a numpy.ndarray with results to get another one with one dimension\n",
    "    probs = predictions.reshape(-1)\n",
    "    #print(outs)\n",
    "    #print(\"probs: \",end=' ')\n",
    "    #print(probs)\n",
    "\n",
    "    # Get an array of 4 class IDs in descending order of probability\n",
    "    top_4 = np.argsort(probs)[-4:][::-1]\n",
    "    print(top_4)\n",
    "\n",
    "    #res = res[output_node_name]\n",
    "    #idx = np.argsort(res[0])[-1]\n",
    "    #prob = res[0][idx]*100\n",
    "\n",
    "    header = 'class_id            probability'\n",
    "    print(f'Image path: {image_path}')\n",
    "    print('Top 4 results: ')\n",
    "    print(header)\n",
    "    print('-' * len(header))\n",
    "\n",
    "    for class_id in top_4:\n",
    "        probability_indent = ' ' * (len('class_id           ') - len(labels[class_id]) + 1)\n",
    "        print(f'{labels[class_id]}{probability_indent}{probs[class_id]*100:.7f}')\n",
    "\n",
    "    #print(labels[top_4[0]], probs[top_4[0]]*100)\n",
    "    #print(labels[top_4[1]], probs[top_4[1]]*100)\n",
    "    #result.append(top_4[0])\n",
    "    #res.write(file+':'+str(labels[top_4[0]])+'\\n')\n",
    "    res.write(file+':top1 = ('+labels[top_4[0]]+')'+str(probs[top_4[0]]*100)+', top2 = ('+labels[top_4[1]]+')'+str(probs[top_4[1]]*100)+'\\n')\n",
    "\n",
    "#res.write(str(result)+'\\n')\n",
    "res.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fp_mlfow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
